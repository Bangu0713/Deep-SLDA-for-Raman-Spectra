{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-02 01:13:58.326217: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-08-02 01:13:58.326246: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-08-02 01:13:58.326960: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "from boxsers.preprocessing import savgol_smoothing, cosmic_filter\n",
    "\n",
    "def preprocessing_method(x):\n",
    "    # 1) Applies a median filter to remove cosmic rays from the spectrum(s).\n",
    "    x = cosmic_filter(x, ks=3)\n",
    "    # 2) Smoothes the spectra\n",
    "    x = savgol_smoothing(x, 7, p=3, degree=0)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_fn = \"./data/Bacteria-ID/X_reference.npy\"\n",
    "y_fn = \"./data/Bacteria-ID/y_reference.npy\"\n",
    "X_train_raw = np.load(X_fn)\n",
    "y_train_raw = np.load(y_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = \"./data/Bacteria-ID/X_test.npy\"\n",
    "y_test = \"./data/Bacteria-ID/y_test.npy\"\n",
    "X_test = np.load(X_test)\n",
    "y_test = np.load(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.Variant_LeNet_without_linear import Variant_LeNet_without_linear\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torch.autograd import Variable\n",
    "from functools import partial\n",
    "from deep_SLDA import slda_loss, SLDA\n",
    "from imblearn.metrics import specificity_score\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "import torch.optim as optim\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import math\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    auc,\n",
    "    roc_curve,\n",
    ")\n",
    "from plot import plot_loss_metrics, plot_metrics\n",
    "\n",
    "n_classes = 30\n",
    "batch_size = 800\n",
    "\n",
    "train_avg_accuracy = []\n",
    "val_avg_accuracy = []\n",
    "avg_accuracy = []\n",
    "avg_roc = []\n",
    "C = np.zeros((30, 30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Solver:\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataloaders,\n",
    "        model,\n",
    "        model_path,\n",
    "        device,\n",
    "        n_classes,\n",
    "    ):\n",
    "        self.dataloaders = dataloaders\n",
    "        self.device = device\n",
    "        self.net = model\n",
    "        self.net = self.net.to(self.device)\n",
    "\n",
    "        self.criterion = partial(\n",
    "            slda_loss,\n",
    "            n_classes=n_classes,\n",
    "        )\n",
    "\n",
    "        self.optimizer = optim.Adam(self.net.parameters(), lr=1e-4, betas=(0.5, 0.999))\n",
    "        self.model_path = model_path\n",
    "        self.n_classes = n_classes\n",
    "        self.slda_layer = SLDA(self.n_classes)\n",
    "\n",
    "    def iterate(self, epoch, phase, scheduler=None):\n",
    "        if phase == \"train\":\n",
    "            self.net.train()\n",
    "        else:\n",
    "            self.net.eval()\n",
    "\n",
    "        dataloader = self.dataloaders[phase]\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        loss_total = 0\n",
    "\n",
    "        # if phase == \"train\":\n",
    "        #     self.optimizer.zero_grad()\n",
    "\n",
    "        for batch_idx, (inputs, targets) in enumerate(dataloader):\n",
    "            inputs, targets = Variable(inputs).to(self.device), Variable(\n",
    "                targets.long()\n",
    "            ).to(self.device)\n",
    "\n",
    "            feas = self.net(inputs)\n",
    "\n",
    "            if phase == \"train\":\n",
    "                dirs, range_eigenvalue, null_eigenvalue = self.slda_layer.fit(feas, targets, phase)\n",
    "                Z = torch.matmul(feas, dirs.T)\n",
    "                self.clf = LinearDiscriminantAnalysis()\n",
    "                self.clf.fit(Z.detach().data.cpu().numpy(),targets.cpu().numpy())\n",
    "                outputs = self.clf.predict(Z.detach().data.cpu().numpy())\n",
    "                outputs = torch.from_numpy(outputs).to(self.device)\n",
    "                loss = self.criterion(range_eigenvalue, null_eigenvalue)\n",
    "                self.dirs = dirs\n",
    "            else:\n",
    "                range_eigenvalue, null_eigenvalue = self.slda_layer.fit(feas, targets, phase)\n",
    "                Z = torch.matmul(feas, self.dirs.T)\n",
    "                outputs = self.clf.predict(Z.detach().data.cpu().numpy())\n",
    "                outputs = torch.from_numpy(outputs).to(self.device)\n",
    "                loss = self.criterion(range_eigenvalue, null_eigenvalue)\n",
    "\n",
    "            if phase == \"train\":\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total += targets.size(0)\n",
    "            loss_total += 1\n",
    "            correct += outputs.eq(targets).cpu().sum().item()\n",
    "            \n",
    "        avg_loss = total_loss / loss_total\n",
    "        total_acc = correct / total\n",
    "\n",
    "        return avg_loss, total_acc\n",
    "\n",
    "    def train(self, epochs):\n",
    "\n",
    "        best_acc = 0\n",
    "\n",
    "        useful_stuff = {\n",
    "            \"training_loss\": [],\n",
    "            \"validation_loss\": [],\n",
    "            \"train_metrics\": [],\n",
    "            \"validation_metrics\": [],\n",
    "        }\n",
    "\n",
    "        lambda1 = lambda epoch: 0.9 ** (epoch // 10) if epoch >= 10 else 1.0\n",
    "        self.scheduler = torch.optim.lr_scheduler.LambdaLR(self.optimizer, lr_lambda=lambda1)\n",
    "\n",
    "        for epoch in tqdm(range(epochs)):\n",
    "            \n",
    "            train_loss, train_acc = self.iterate(epoch, \"train\")\n",
    "            useful_stuff[\"training_loss\"].append(train_loss)\n",
    "            useful_stuff[\"train_metrics\"].append(train_acc)\n",
    "\n",
    "            # self.optimizer.step()\n",
    "            self.scheduler.step()\n",
    "            # print(epoch, self.scheduler.get_last_lr()[0])\n",
    "\n",
    "            with torch.no_grad():\n",
    "                val_loss, val_acc = self.iterate(epoch, \"val\")\n",
    "                useful_stuff[\"validation_loss\"].append(val_loss)\n",
    "                useful_stuff[\"validation_metrics\"].append(val_acc)\n",
    "\n",
    "            if val_acc > best_acc or epoch == 0:\n",
    "                best_acc = val_acc\n",
    "                checkpoint = {\n",
    "                    \"epoch\": epoch,\n",
    "                    \"val_loss\": val_loss,\n",
    "                    \"dirs\": self.dirs,\n",
    "                    \"clf\": self.clf,\n",
    "                    \"state_dict\": self.net.state_dict(),\n",
    "                }\n",
    "                torch.save(checkpoint, self.model_path)\n",
    "\n",
    "        return train_acc, best_acc, useful_stuff\n",
    "\n",
    "    def test_iterate(self, epoch, phase):\n",
    "        self.net.eval()\n",
    "        dataloader = self.dataloaders[phase]\n",
    "        y_pred = []\n",
    "        y_true = []\n",
    "        y_pred_prob = []\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (inputs, targets) in enumerate(dataloader):\n",
    "                inputs, targets = Variable(inputs.cuda()), Variable(\n",
    "                    targets.cuda().long()\n",
    "                )\n",
    "\n",
    "                feas = self.net(inputs)\n",
    "                Z = torch.matmul(feas, self.dirs.T)\n",
    "                outputs = self.clf.predict(Z.detach().data.cpu().numpy())\n",
    "                outputs = torch.from_numpy(outputs).to(self.device)\n",
    "                outputs_prob = self.clf.predict_proba(Z.detach().data.cpu().numpy())\n",
    "                outputs_prob = torch.from_numpy(outputs_prob).to(self.device)\n",
    "                    \n",
    "                y_pred.append(outputs.cpu().numpy().ravel())\n",
    "                y_true.append(targets.cpu().numpy())\n",
    "                y_pred_prob.append(outputs_prob.cpu().numpy())\n",
    "            pass\n",
    "        \n",
    "        y_pred_prob = np.concatenate(y_pred_prob)\n",
    "        y_pred = np.concatenate(y_pred)\n",
    "        y_true = np.concatenate(y_true)\n",
    "\n",
    "        return (\n",
    "            np.array(y_pred).flatten(),\n",
    "            np.array(y_true).flatten(),\n",
    "            np.array(y_pred_prob).reshape(3000, 30),\n",
    "        )\n",
    "\n",
    "    def test(self):\n",
    "        checkpoint = torch.load(self.model_path)\n",
    "        epoch = checkpoint[\"epoch\"]\n",
    "        val_loss = checkpoint[\"val_loss\"]\n",
    "        self.dirs = checkpoint[\"dirs\"]\n",
    "        self.clf = checkpoint[\"clf\"]\n",
    "\n",
    "        self.net.load_state_dict(checkpoint[\"state_dict\"])\n",
    "        print(\"load model at epoch {}, with val loss: {:.3f}\".format(epoch, val_loss))\n",
    "        y_pred, y_true, y_pred_prob = self.test_iterate(epoch, \"test\")\n",
    "        print(\"total\", accuracy_score(y_true, y_pred))\n",
    "        for i in range(self.n_classes):\n",
    "            idx = y_true == i\n",
    "            print(\"class\", i, accuracy_score(y_true[idx], y_pred[idx]))\n",
    "\n",
    "        return (\n",
    "            confusion_matrix(y_true, y_pred),\n",
    "            y_true,\n",
    "            y_pred,\n",
    "            accuracy_score(y_true, y_pred),\n",
    "            y_pred_prob,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import sampler\n",
    "\n",
    "class StratifiedSampler(sampler.Sampler):\n",
    "    \"\"\"\n",
    "    Stratified Sampling: Provides equal representation of target classes in each batch\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, class_vector: torch.tensor, batch_size: int):\n",
    "        \"\"\"\n",
    "\n",
    "        Args:\n",
    "            class_vector (torch.tensor): A vector of class labels.\n",
    "            batch_size (int): Batch size.\n",
    "        \"\"\"\n",
    "        self.n_splits = int(class_vector.size / batch_size)\n",
    "        self.class_vector = class_vector\n",
    "\n",
    "    def gen_sample_array(self):\n",
    "        from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "        s = StratifiedShuffleSplit(n_splits=self.n_splits, test_size=0.5)\n",
    "        X = torch.randn(self.class_vector.size, 2).numpy()\n",
    "        y = self.class_vector\n",
    "        s.get_n_splits(X, y)\n",
    "\n",
    "        train_index, test_index = next(s.split(X, y))\n",
    "        return np.hstack([train_index, test_index])\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self.gen_sample_array())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.class_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "train size:  48000\n",
      "validation size:  12000\n",
      "test size:  3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [1:17:58<00:00, 23.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load model at epoch 129, with val loss: 4.245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 0.4663333333333333\n",
      "class 0 0.86\n",
      "class 1 0.0\n",
      "class 2 0.01\n",
      "class 3 0.99\n",
      "class 4 0.42\n",
      "class 5 1.0\n",
      "class 6 0.64\n",
      "class 7 0.59\n",
      "class 8 0.03\n",
      "class 9 0.0\n",
      "class 10 0.45\n",
      "class 11 0.06\n",
      "class 12 0.14\n",
      "class 13 0.01\n",
      "class 14 1.0\n",
      "class 15 1.0\n",
      "class 16 0.0\n",
      "class 17 0.99\n",
      "class 18 0.77\n",
      "class 19 0.94\n",
      "class 20 0.86\n",
      "class 21 0.0\n",
      "class 22 0.01\n",
      "class 23 0.87\n",
      "class 24 0.0\n",
      "class 25 0.7\n",
      "class 26 0.41\n",
      "class 27 0.86\n",
      "class 28 0.07\n",
      "class 29 0.31\n",
      "                                                             0\n",
      "Accuracy                                                0.4663\n",
      "Recall       [0.86, 0.0, 0.01, 0.99, 0.42, 1.0, 0.64, 0.59,...\n",
      "Specificity  [0.9659, 1.0, 0.9972, 0.9817, 0.9931, 0.9941, ...\n",
      "Precision    [0.4649, 0.0, 0.1111, 0.6513, 0.6774, 0.8547, ...\n",
      "F1 Score     [0.6035, 0.0, 0.0183, 0.7857, 0.5185, 0.9217, ...\n",
      "2\n",
      "train size:  48000\n",
      "validation size:  12000\n",
      "test size:  3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [1:19:00<00:00, 23.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load model at epoch 129, with val loss: 4.247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 0.48966666666666664\n",
      "class 0 0.81\n",
      "class 1 0.0\n",
      "class 2 0.05\n",
      "class 3 1.0\n",
      "class 4 0.27\n",
      "class 5 1.0\n",
      "class 6 0.63\n",
      "class 7 0.67\n",
      "class 8 0.02\n",
      "class 9 0.03\n",
      "class 10 0.51\n",
      "class 11 0.09\n",
      "class 12 0.2\n",
      "class 13 0.0\n",
      "class 14 0.96\n",
      "class 15 1.0\n",
      "class 16 0.0\n",
      "class 17 0.99\n",
      "class 18 0.85\n",
      "class 19 1.0\n",
      "class 20 0.97\n",
      "class 21 0.0\n",
      "class 22 0.04\n",
      "class 23 0.87\n",
      "class 24 0.0\n",
      "class 25 0.63\n",
      "class 26 0.47\n",
      "class 27 0.81\n",
      "class 28 0.1\n",
      "class 29 0.72\n",
      "                                                             0\n",
      "Accuracy                                                0.4897\n",
      "Recall       [0.81, 0.0, 0.05, 1.0, 0.27, 1.0, 0.63, 0.67, ...\n",
      "Specificity  [0.9659, 1.0, 0.9997, 0.9752, 0.9921, 0.9969, ...\n",
      "Precision    [0.45, 0.0, 0.8333, 0.5814, 0.54, 0.9174, 0.34...\n",
      "F1 Score     [0.5786, 0.0, 0.0943, 0.7353, 0.36, 0.9569, 0....\n",
      "3\n",
      "train size:  48000\n",
      "validation size:  12000\n",
      "test size:  3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [1:18:29<00:00, 23.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load model at epoch 155, with val loss: 4.218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 0.49066666666666664\n",
      "class 0 0.81\n",
      "class 1 0.0\n",
      "class 2 0.17\n",
      "class 3 1.0\n",
      "class 4 0.31\n",
      "class 5 1.0\n",
      "class 6 0.7\n",
      "class 7 0.71\n",
      "class 8 0.05\n",
      "class 9 0.0\n",
      "class 10 0.55\n",
      "class 11 0.0\n",
      "class 12 0.12\n",
      "class 13 0.01\n",
      "class 14 1.0\n",
      "class 15 1.0\n",
      "class 16 0.01\n",
      "class 17 0.96\n",
      "class 18 0.82\n",
      "class 19 0.94\n",
      "class 20 0.96\n",
      "class 21 0.0\n",
      "class 22 0.03\n",
      "class 23 0.94\n",
      "class 24 0.0\n",
      "class 25 0.47\n",
      "class 26 0.66\n",
      "class 27 0.89\n",
      "class 28 0.06\n",
      "class 29 0.55\n",
      "                                                             0\n",
      "Accuracy                                                0.4907\n",
      "Recall       [0.81, 0.0, 0.17, 1.0, 0.31, 1.0, 0.7, 0.71, 0...\n",
      "Specificity  [0.9679, 1.0, 0.9866, 0.99, 0.9976, 0.991, 0.9...\n",
      "Precision    [0.4655, 0.0, 0.3036, 0.7752, 0.8158, 0.7937, ...\n",
      "F1 Score     [0.5912, 0.0, 0.2179, 0.8734, 0.4493, 0.885, 0...\n",
      "4\n",
      "train size:  48000\n",
      "validation size:  12000\n",
      "test size:  3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [1:18:43<00:00, 23.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load model at epoch 169, with val loss: 4.213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 0.48833333333333334\n",
      "class 0 0.84\n",
      "class 1 0.0\n",
      "class 2 0.01\n",
      "class 3 0.99\n",
      "class 4 0.74\n",
      "class 5 1.0\n",
      "class 6 0.67\n",
      "class 7 0.57\n",
      "class 8 0.12\n",
      "class 9 0.02\n",
      "class 10 0.34\n",
      "class 11 0.01\n",
      "class 12 0.18\n",
      "class 13 0.01\n",
      "class 14 0.98\n",
      "class 15 0.99\n",
      "class 16 0.01\n",
      "class 17 0.97\n",
      "class 18 0.77\n",
      "class 19 0.96\n",
      "class 20 0.77\n",
      "class 21 0.0\n",
      "class 22 0.01\n",
      "class 23 0.92\n",
      "class 24 0.0\n",
      "class 25 0.67\n",
      "class 26 0.61\n",
      "class 27 0.9\n",
      "class 28 0.12\n",
      "class 29 0.47\n",
      "                                                             0\n",
      "Accuracy                                                0.4883\n",
      "Recall       [0.84, 0.0, 0.01, 0.99, 0.74, 1.0, 0.67, 0.57,...\n",
      "Specificity  [0.9721, 1.0, 1.0, 0.9941, 0.9807, 0.9869, 0.9...\n",
      "Precision    [0.5091, 0.0, 1.0, 0.8534, 0.5692, 0.7246, 0.4...\n",
      "F1 Score     [0.634, 0.0, 0.0198, 0.9167, 0.6435, 0.8403, 0...\n",
      "5\n",
      "train size:  48000\n",
      "validation size:  12000\n",
      "test size:  3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [1:18:48<00:00, 23.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load model at epoch 171, with val loss: 4.233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 0.4613333333333333\n",
      "class 0 0.81\n",
      "class 1 0.0\n",
      "class 2 0.03\n",
      "class 3 1.0\n",
      "class 4 0.32\n",
      "class 5 1.0\n",
      "class 6 0.76\n",
      "class 7 0.41\n",
      "class 8 0.02\n",
      "class 9 0.01\n",
      "class 10 0.57\n",
      "class 11 0.0\n",
      "class 12 0.16\n",
      "class 13 0.02\n",
      "class 14 0.98\n",
      "class 15 1.0\n",
      "class 16 0.03\n",
      "class 17 0.98\n",
      "class 18 0.77\n",
      "class 19 0.92\n",
      "class 20 0.96\n",
      "class 21 0.0\n",
      "class 22 0.01\n",
      "class 23 0.95\n",
      "class 24 0.0\n",
      "class 25 0.35\n",
      "class 26 0.64\n",
      "class 27 0.89\n",
      "class 28 0.1\n",
      "class 29 0.15\n",
      "                                                             0\n",
      "Accuracy                                                0.4613\n",
      "Recall       [0.81, 0.0, 0.03, 1.0, 0.32, 1.0, 0.76, 0.41, ...\n",
      "Specificity  [0.9679, 1.0, 0.9983, 0.9769, 0.9941, 0.9897, ...\n",
      "Precision    [0.4655, 0.0, 0.375, 0.5988, 0.6531, 0.7692, 0...\n",
      "F1 Score     [0.5912, 0.0, 0.0556, 0.7491, 0.4295, 0.8696, ...\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from datasets_spectrum import spectral_dataloader\n",
    "from config import ORDER, STRAINS\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
    "fold_index = 1\n",
    "\n",
    "for train_idx, valid_idx in kfold.split(X_train_raw, y_train_raw):\n",
    "\n",
    "    print(fold_index)\n",
    "    x_train, y_train = X_train_raw[train_idx], y_train_raw[train_idx]\n",
    "    x_valid, y_valid = X_train_raw[valid_idx], y_train_raw[valid_idx]\n",
    "    \n",
    "    print(\"train size: \", len(x_train))\n",
    "    print(\"validation size: \", len(x_valid))\n",
    "    print(\"test size: \", len(y_test))\n",
    "\n",
    "    stratified_train_batch_sampler = StratifiedSampler(y_train, batch_size)\n",
    "    dl_tr = spectral_dataloader(\n",
    "        x_train, y_train, idxs=None, batch_size=batch_size, shuffle=False,sampler=stratified_train_batch_sampler,\n",
    "    )\n",
    "    stratified_train_batch_sampler = StratifiedSampler(y_valid, batch_size)\n",
    "    dl_val = spectral_dataloader(\n",
    "        x_valid, y_valid, idxs=None, batch_size=batch_size, shuffle=False,sampler=stratified_train_batch_sampler,\n",
    "    )\n",
    "    dl_test = spectral_dataloader(X_test, y_test, batch_size=batch_size, shuffle=False)\n",
    "    values, counts = np.unique(np.asarray(y_test), return_counts=True)\n",
    "\n",
    "    dataloaders = {\"train\": dl_tr, \"val\": dl_val, \"test\": dl_test}\n",
    "    model = Variant_LeNet_without_linear(in_channels=1)\n",
    "\n",
    "    model_path = f\"best_variant_lenet_model_{fold_index}.pt\"\n",
    "    solver = Solver(\n",
    "        dataloaders, model, model_path, \"cuda\", n_classes\n",
    "    )\n",
    "    train_accuracy, val_accuracy, useful_stuff = solver.train(200)\n",
    "    C, y_true, y_pred, test_accuracy, y_pred_prob = solver.test()\n",
    "    train_avg_accuracy.append(train_accuracy)\n",
    "    val_avg_accuracy.append(val_accuracy)\n",
    "    avg_accuracy.append(test_accuracy)\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    for i in range(np.unique(y_true).shape[0]):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_test == i, y_pred_prob[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "    values = [\n",
    "        v\n",
    "        for v in roc_auc.values()\n",
    "        if isinstance(v, (int, float)) and not math.isnan(v)\n",
    "    ]\n",
    "    if values:\n",
    "        auc_score = sum(values) / len(values)\n",
    "    avg_roc.append(auc_score)\n",
    "\n",
    "    # Plot confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=ORDER)\n",
    "    sns.set_context(\"talk\", rc={\"font\": \"Helvetica\", \"font.size\": 12})\n",
    "    label = [STRAINS[i] for i in ORDER]\n",
    "    cm = 100 * cm / cm.sum(axis=1)[:,np.newaxis]\n",
    "    \n",
    "    # calculate comfusion matrix\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    sensitivity = recall_score(y_true, y_pred, average=\"micro\", zero_division=0)\n",
    "    specificity = cm[1, 1] / (cm[1, 0] + cm[1, 1])\n",
    "    f1 = f1_score(y_true, y_pred, average=\"micro\", zero_division=0)\n",
    "\n",
    "    df = pd.DataFrame(\n",
    "        {\n",
    "            \"Accuracy\": [np.round(accuracy_score(y_true, y_pred), 4)],\n",
    "            \"Recall\": [\n",
    "                recall_score(y_true, y_pred, average=None, zero_division=0).round(4)\n",
    "            ],\n",
    "            \"Specificity\": [specificity_score(y_true, y_pred, average=None).round(4)],\n",
    "            \"Precision\": [\n",
    "                precision_score(y_true, y_pred, average=None, zero_division=0).round(4)\n",
    "            ],\n",
    "            \"F1 Score\": [\n",
    "                f1_score(y_true, y_pred, average=None, zero_division=0).round(4)\n",
    "            ],\n",
    "        }\n",
    "    )\n",
    "    print(df.transpose())\n",
    "\n",
    "    plot_metrics(training_results=useful_stuff, fold_index=fold_index, fold_name=\"variant_lenet\")\n",
    "    plot_loss_metrics(training_results=useful_stuff, fold_index=fold_index, fold_name=\"variant_lenet\")\n",
    "\n",
    "    fold_index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train mean: 1.0\n",
      "train std: 0.0\n",
      "val mean: 0.9362\n",
      "val std: 0.0011\n",
      "test mean: 0.4793\n",
      "test std: 0.0127\n",
      "auc mean: 0.8523\n",
      "auc std: 0.0088\n"
     ]
    }
   ],
   "source": [
    "print(\"train mean:\", round(np.mean(train_avg_accuracy),4))\n",
    "print(\"train std:\", round(np.std(train_avg_accuracy),4))\n",
    "\n",
    "print(\"val mean:\", round(np.mean(val_avg_accuracy),4))\n",
    "print(\"val std:\", round(np.std(val_avg_accuracy),4))\n",
    "\n",
    "print(\"test mean:\", round(np.mean(avg_accuracy),4))\n",
    "print(\"test std:\", round(np.std(avg_accuracy),4))\n",
    "\n",
    "print(\"auc mean:\", round(np.mean(avg_roc),4))\n",
    "print(\"auc std:\", round(np.std(avg_roc),4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
