{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-06 12:16:59.039451: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-08-06 12:16:59.039477: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-08-06 12:16:59.040157: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "from boxsers.preprocessing import savgol_smoothing, cosmic_filter\n",
    "\n",
    "def preprocessing_method(x):\n",
    "    # 1) Applies a median filter to remove cosmic rays from the spectrum(s).\n",
    "    x = cosmic_filter(x, ks=3)\n",
    "    # 2) Smoothes the spectra\n",
    "    x = savgol_smoothing(x, 7, p=3, degree=0)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_fn = \"./data/Bacteria-ID/X_finetune.npy\"\n",
    "y_fn = \"./data/Bacteria-ID/y_finetune.npy\"\n",
    "X_train_raw = np.load(X_fn)\n",
    "y_train_raw = np.load(y_fn)\n",
    "X_train_raw = preprocessing_method(X_train_raw)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = \"./data/Bacteria-ID/X_test.npy\"\n",
    "y_test = \"./data/Bacteria-ID/y_test.npy\"\n",
    "X_test = np.load(X_test)\n",
    "y_test = np.load(y_test)\n",
    "X_test = preprocessing_method(X_test)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.Variant_LeNet_without_linear import Variant_LeNet_without_linear\n",
    "from model.Variant_LeNet import Variant_LeNet\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from functools import partial\n",
    "from deep_SLDA import slda_loss, SLDA\n",
    "from imblearn.metrics import specificity_score\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "import torch.optim as optim\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    auc,\n",
    "    roc_curve,\n",
    ")\n",
    "from plot import plot_ROC_curve, plot_heatmap, plot_loss_metrics, plot_metrics, plot_antibiotic_groupings\n",
    "\n",
    "n_classes = 30\n",
    "batch_size = 3000\n",
    "\n",
    "antibiotic_accuracy = []\n",
    "train_avg_accuracy = []\n",
    "val_avg_accuracy = []\n",
    "avg_accuracy = []\n",
    "avg_recall = []\n",
    "avg_specificity = []\n",
    "avg_precision = []\n",
    "avg_f1 = []\n",
    "avg_roc = []\n",
    "C = np.zeros((30, 30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Solver:\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataloaders,\n",
    "        model,\n",
    "        model_path,\n",
    "        device,\n",
    "        n_classes,\n",
    "        finetune_model\n",
    "    ):\n",
    "        self.dataloaders = dataloaders\n",
    "        self.device = device\n",
    "        self.net = model\n",
    "        self.net.load_state_dict(finetune_model[\"state_dict\"], strict=False)\n",
    "        self.net = self.net.to(self.device)\n",
    "\n",
    "        self.criterion = partial(\n",
    "            slda_loss,\n",
    "            n_classes=n_classes,\n",
    "        )\n",
    "\n",
    "        self.optimizer = optim.Adam(self.net.parameters(), lr=1e-4, betas=(0.5, 0.999))\n",
    "        self.model_path = model_path\n",
    "        self.n_classes = n_classes\n",
    "        self.slda_layer = SLDA(self.n_classes)\n",
    "\n",
    "    def iterate(self, epoch, phase, scheduler=None):\n",
    "        if phase == \"train\":\n",
    "            self.net.train()\n",
    "        else:\n",
    "            self.net.eval()\n",
    "\n",
    "        dataloader = self.dataloaders[phase]\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        loss_total = 0\n",
    "\n",
    "        for batch_idx, (inputs, targets) in enumerate(dataloader):\n",
    "            inputs, targets = Variable(inputs).to(self.device), Variable(\n",
    "                targets.long()\n",
    "            ).to(self.device)\n",
    "            \n",
    "            feas = self.net(inputs)\n",
    "            \n",
    "            if phase == \"train\":\n",
    "                dirs, range_eigenvalue, null_eigenvalue = self.slda_layer.fit(feas, targets, phase)\n",
    "                Z = torch.matmul(feas, dirs.T)\n",
    "                self.clf = LinearDiscriminantAnalysis()\n",
    "                self.clf.fit(Z.detach().data.cpu().numpy(),targets.cpu().numpy())\n",
    "                outputs = self.clf.predict(Z.detach().data.cpu().numpy())\n",
    "                outputs = torch.from_numpy(outputs).to(self.device)\n",
    "                loss = self.criterion(range_eigenvalue, null_eigenvalue)\n",
    "                self.dirs = dirs\n",
    "            else:\n",
    "                range_eigenvalue, null_eigenvalue = self.slda_layer.fit(feas, targets, phase)\n",
    "                Z = torch.matmul(feas, self.dirs.T)\n",
    "                outputs = self.clf.predict(Z.detach().data.cpu().numpy())\n",
    "                outputs = torch.from_numpy(outputs).to(self.device)\n",
    "                loss = self.criterion(range_eigenvalue, null_eigenvalue)\n",
    "            \n",
    "            if phase == \"train\":\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total += targets.size(0)\n",
    "            loss_total += 1\n",
    "            correct += outputs.eq(targets).cpu().sum().item()\n",
    "\n",
    "        avg_loss = total_loss / loss_total\n",
    "        total_acc = correct / total\n",
    "\n",
    "        return avg_loss, total_acc\n",
    "\n",
    "    def train(self, epochs):\n",
    "\n",
    "        best_acc = 0\n",
    "\n",
    "        useful_stuff = {\n",
    "            \"training_loss\": [],\n",
    "            \"validation_loss\": [],\n",
    "            \"train_metrics\": [],\n",
    "            \"validation_metrics\": [],\n",
    "        }\n",
    "\n",
    "        lambda1 = lambda epoch: 0.9 ** (epoch // 10) if epoch >= 10 else 1.0\n",
    "        self.scheduler = torch.optim.lr_scheduler.LambdaLR(self.optimizer, lr_lambda=lambda1)\n",
    "\n",
    "        for epoch in tqdm(range(epochs)):\n",
    "            \n",
    "            train_loss, train_acc = self.iterate(epoch, \"train\")\n",
    "            useful_stuff[\"training_loss\"].append(train_loss)\n",
    "            useful_stuff[\"train_metrics\"].append(train_acc)\n",
    "\n",
    "            # self.optimizer.step()\n",
    "            self.scheduler.step()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                val_loss, val_acc = self.iterate(epoch, \"val\")\n",
    "                useful_stuff[\"validation_loss\"].append(val_loss)\n",
    "                useful_stuff[\"validation_metrics\"].append(val_acc)\n",
    "\n",
    "            if val_acc > best_acc or epoch == 0:\n",
    "                best_acc = val_acc\n",
    "                checkpoint = {\n",
    "                    \"epoch\": epoch,\n",
    "                    \"val_loss\": val_loss,\n",
    "                    \"dirs\": self.dirs,\n",
    "                    \"clf\": self.clf,\n",
    "                    \"state_dict\": self.net.state_dict(),\n",
    "                }\n",
    "                torch.save(checkpoint, self.model_path)  \n",
    "\n",
    "        return train_acc, best_acc, useful_stuff\n",
    "\n",
    "    def test_iterate(self, epoch, phase):\n",
    "        self.net.eval()\n",
    "        dataloader = self.dataloaders[phase]\n",
    "        y_pred = []\n",
    "        y_true = []\n",
    "        y_pred_prob = []\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (inputs, targets) in enumerate(dataloader):\n",
    "                inputs, targets = Variable(inputs.cuda()), Variable(\n",
    "                    targets.cuda().long()\n",
    "                )\n",
    "                \n",
    "                feas = self.net(inputs)\n",
    "                Z = torch.matmul(feas, self.dirs.T)\n",
    "                outputs = self.clf.predict(Z.detach().data.cpu().numpy())\n",
    "                outputs = torch.from_numpy(outputs).to(self.device)\n",
    "                outputs_prob = self.clf.predict_proba(Z.detach().data.cpu().numpy())\n",
    "                outputs_prob = torch.from_numpy(outputs_prob).to(self.device)\n",
    "\n",
    "                y_pred.append(outputs.cpu().numpy().ravel())\n",
    "                y_true.append(targets.cpu().numpy())\n",
    "                y_pred_prob.append(outputs_prob.cpu().numpy())\n",
    "            pass\n",
    "        \n",
    "        y_pred_prob = np.concatenate(y_pred_prob)\n",
    "        y_pred = np.concatenate(y_pred)\n",
    "        y_true = np.concatenate(y_true)\n",
    "\n",
    "        return (\n",
    "            np.array(y_pred).flatten(),\n",
    "            np.array(y_true).flatten(),\n",
    "            np.array(y_pred_prob).reshape(3000, 30),\n",
    "        )\n",
    "\n",
    "    def test(self):\n",
    "        checkpoint = torch.load(self.model_path)\n",
    "        epoch = checkpoint[\"epoch\"]\n",
    "        val_loss = checkpoint[\"val_loss\"]\n",
    "        self.dirs = checkpoint[\"dirs\"]\n",
    "        self.clf = checkpoint[\"clf\"]\n",
    "        self.net.load_state_dict(checkpoint[\"state_dict\"])\n",
    "        print(\"load model at epoch {}, with val loss: {:.3f}\".format(epoch, val_loss))\n",
    "        y_pred, y_true, y_pred_prob = self.test_iterate(epoch, \"test\")\n",
    "        print(\"total\", accuracy_score(y_true, y_pred))\n",
    "        for i in range(self.n_classes):\n",
    "            idx = y_true == i\n",
    "            print(\"class\", i, accuracy_score(y_true[idx], y_pred[idx]))\n",
    "\n",
    "        return (\n",
    "            confusion_matrix(y_true, y_pred),\n",
    "            y_true,\n",
    "            y_pred,\n",
    "            accuracy_score(y_true, y_pred),\n",
    "            y_pred_prob,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold:  1\n",
      "train size:  2700\n",
      "validation size:  300\n",
      "test size:  3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [04:21<00:00,  1.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load model at epoch 16, with val loss: 0.224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 0.8533333333333334\n",
      "class 0 0.96\n",
      "class 1 1.0\n",
      "class 2 0.67\n",
      "class 3 1.0\n",
      "class 4 0.66\n",
      "class 5 1.0\n",
      "class 6 0.43\n",
      "class 7 0.87\n",
      "class 8 0.54\n",
      "class 9 0.7\n",
      "class 10 0.92\n",
      "class 11 0.37\n",
      "class 12 0.97\n",
      "class 13 0.81\n",
      "class 14 1.0\n",
      "class 15 0.86\n",
      "class 16 0.88\n",
      "class 17 0.7\n",
      "class 18 0.94\n",
      "class 19 0.99\n",
      "class 20 1.0\n",
      "class 21 0.92\n",
      "class 22 0.89\n",
      "class 23 0.87\n",
      "class 24 0.88\n",
      "class 25 0.91\n",
      "class 26 0.95\n",
      "class 27 0.99\n",
      "class 28 0.96\n",
      "class 29 0.96\n",
      "                                                             0\n",
      "Accuracy                                                0.8533\n",
      "Recall       [0.96, 1.0, 0.67, 1.0, 0.66, 1.0, 0.43, 0.87, ...\n",
      "Specificity  [1.0, 0.9986, 0.9893, 0.9966, 0.9928, 0.999, 0...\n",
      "Precision    [1.0, 0.9615, 0.6837, 0.9091, 0.7586, 0.9709, ...\n",
      "F1 Score     [0.9796, 0.9804, 0.6768, 0.9524, 0.7059, 0.985...\n",
      "Accuracy: 97.3%\n",
      "fold:  2\n",
      "train size:  2700\n",
      "validation size:  300\n",
      "test size:  3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [04:25<00:00,  1.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load model at epoch 11, with val loss: 0.211\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 0.8573333333333333\n",
      "class 0 0.95\n",
      "class 1 1.0\n",
      "class 2 0.7\n",
      "class 3 1.0\n",
      "class 4 0.66\n",
      "class 5 1.0\n",
      "class 6 0.5\n",
      "class 7 0.82\n",
      "class 8 0.61\n",
      "class 9 0.67\n",
      "class 10 0.92\n",
      "class 11 0.39\n",
      "class 12 0.97\n",
      "class 13 0.79\n",
      "class 14 1.0\n",
      "class 15 0.79\n",
      "class 16 0.95\n",
      "class 17 0.76\n",
      "class 18 0.94\n",
      "class 19 0.99\n",
      "class 20 0.99\n",
      "class 21 0.94\n",
      "class 22 0.89\n",
      "class 23 0.88\n",
      "class 24 0.86\n",
      "class 25 0.91\n",
      "class 26 0.92\n",
      "class 27 0.99\n",
      "class 28 0.96\n",
      "class 29 0.97\n",
      "                                                             0\n",
      "Accuracy                                                0.8573\n",
      "Recall       [0.95, 1.0, 0.7, 1.0, 0.66, 1.0, 0.5, 0.82, 0....\n",
      "Specificity  [1.0, 0.9983, 0.9879, 0.9972, 0.99, 0.999, 0.9...\n",
      "Precision    [1.0, 0.9524, 0.6667, 0.9259, 0.6947, 0.9709, ...\n",
      "F1 Score     [0.9744, 0.9756, 0.6829, 0.9615, 0.6769, 0.985...\n",
      "Accuracy: 97.3%\n",
      "fold:  3\n",
      "train size:  2700\n",
      "validation size:  300\n",
      "test size:  3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [04:31<00:00,  1.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load model at epoch 11, with val loss: 0.204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 0.8536666666666667\n",
      "class 0 0.97\n",
      "class 1 1.0\n",
      "class 2 0.64\n",
      "class 3 1.0\n",
      "class 4 0.66\n",
      "class 5 1.0\n",
      "class 6 0.45\n",
      "class 7 0.84\n",
      "class 8 0.56\n",
      "class 9 0.7\n",
      "class 10 0.92\n",
      "class 11 0.38\n",
      "class 12 0.97\n",
      "class 13 0.81\n",
      "class 14 1.0\n",
      "class 15 0.87\n",
      "class 16 0.89\n",
      "class 17 0.72\n",
      "class 18 0.95\n",
      "class 19 0.98\n",
      "class 20 1.0\n",
      "class 21 0.94\n",
      "class 22 0.87\n",
      "class 23 0.87\n",
      "class 24 0.83\n",
      "class 25 0.91\n",
      "class 26 0.93\n",
      "class 27 1.0\n",
      "class 28 0.98\n",
      "class 29 0.97\n",
      "                                                             0\n",
      "Accuracy                                                0.8537\n",
      "Recall       [0.97, 1.0, 0.64, 1.0, 0.66, 1.0, 0.45, 0.84, ...\n",
      "Specificity  [1.0, 0.999, 0.989, 0.9969, 0.9928, 0.999, 0.9...\n",
      "Precision    [1.0, 0.9709, 0.6667, 0.9174, 0.7586, 0.9709, ...\n",
      "F1 Score     [0.9848, 0.9852, 0.6531, 0.9569, 0.7059, 0.985...\n",
      "Accuracy: 97.3%\n",
      "fold:  4\n",
      "train size:  2700\n",
      "validation size:  300\n",
      "test size:  3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [04:32<00:00,  1.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load model at epoch 37, with val loss: 0.183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 0.8563333333333333\n",
      "class 0 0.97\n",
      "class 1 1.0\n",
      "class 2 0.62\n",
      "class 3 1.0\n",
      "class 4 0.65\n",
      "class 5 1.0\n",
      "class 6 0.44\n",
      "class 7 0.84\n",
      "class 8 0.63\n",
      "class 9 0.69\n",
      "class 10 0.92\n",
      "class 11 0.4\n",
      "class 12 0.97\n",
      "class 13 0.81\n",
      "class 14 1.0\n",
      "class 15 0.83\n",
      "class 16 0.91\n",
      "class 17 0.68\n",
      "class 18 0.94\n",
      "class 19 0.99\n",
      "class 20 1.0\n",
      "class 21 0.94\n",
      "class 22 0.88\n",
      "class 23 0.88\n",
      "class 24 0.88\n",
      "class 25 0.92\n",
      "class 26 0.94\n",
      "class 27 1.0\n",
      "class 28 0.98\n",
      "class 29 0.98\n",
      "                                                             0\n",
      "Accuracy                                                0.8563\n",
      "Recall       [0.97, 1.0, 0.62, 1.0, 0.65, 1.0, 0.44, 0.84, ...\n",
      "Specificity  [1.0, 0.999, 0.989, 0.9972, 0.9931, 0.999, 0.9...\n",
      "Precision    [1.0, 0.9709, 0.6596, 0.9259, 0.7647, 0.9709, ...\n",
      "F1 Score     [0.9848, 0.9852, 0.6392, 0.9615, 0.7027, 0.985...\n",
      "Accuracy: 97.3%\n",
      "fold:  5\n",
      "train size:  2700\n",
      "validation size:  300\n",
      "test size:  3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [04:34<00:00,  1.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load model at epoch 43, with val loss: 0.209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 0.861\n",
      "class 0 0.97\n",
      "class 1 1.0\n",
      "class 2 0.65\n",
      "class 3 1.0\n",
      "class 4 0.65\n",
      "class 5 1.0\n",
      "class 6 0.44\n",
      "class 7 0.87\n",
      "class 8 0.63\n",
      "class 9 0.71\n",
      "class 10 0.91\n",
      "class 11 0.39\n",
      "class 12 0.99\n",
      "class 13 0.85\n",
      "class 14 1.0\n",
      "class 15 0.81\n",
      "class 16 0.94\n",
      "class 17 0.71\n",
      "class 18 0.94\n",
      "class 19 0.99\n",
      "class 20 0.99\n",
      "class 21 0.94\n",
      "class 22 0.88\n",
      "class 23 0.88\n",
      "class 24 0.89\n",
      "class 25 0.92\n",
      "class 26 0.94\n",
      "class 27 1.0\n",
      "class 28 0.97\n",
      "class 29 0.97\n",
      "                                                             0\n",
      "Accuracy                                                 0.861\n",
      "Recall       [0.97, 1.0, 0.65, 1.0, 0.65, 1.0, 0.44, 0.87, ...\n",
      "Specificity  [1.0, 0.999, 0.99, 0.9966, 0.9934, 0.999, 0.99...\n",
      "Precision    [1.0, 0.9709, 0.6915, 0.9091, 0.7738, 0.9709, ...\n",
      "F1 Score     [0.9848, 0.9852, 0.6701, 0.9524, 0.7065, 0.985...\n",
      "Accuracy: 97.4%\n"
     ]
    }
   ],
   "source": [
    "from datasets_spectrum import spectral_dataloader\n",
    "from config import ORDER, STRAINS\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "p_val = 0.1\n",
    "n_val = int(3000 * p_val)\n",
    "idx_tr = list(range(3000))\n",
    "np.random.shuffle(idx_tr)\n",
    "idx_val = idx_tr[:n_val]\n",
    "idx_tr = idx_tr[n_val:]\n",
    "\n",
    "fold_index = 1\n",
    "\n",
    "for i in range(5):\n",
    "\n",
    "    print(\"fold: \", fold_index)\n",
    "    print(\"train size: \", len(idx_tr))\n",
    "    print(\"validation size: \", len(idx_val))\n",
    "    print(\"test size: \", len(y_test))\n",
    "\n",
    "    dl_tr = spectral_dataloader(\n",
    "        X_train_raw, y_train_raw, idxs=idx_tr, batch_size=batch_size, shuffle=True\n",
    "    )\n",
    "    dl_val = spectral_dataloader(\n",
    "        X_train_raw, y_train_raw, idxs=idx_val, batch_size=batch_size, shuffle=False\n",
    "    )\n",
    "    dl_test = spectral_dataloader(X_test, y_test, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    values, counts = np.unique(np.asarray(y_test), return_counts=True)\n",
    "\n",
    "    dataloaders = {\"train\": dl_tr, \"val\": dl_val, \"test\": dl_test}\n",
    "    model = Variant_LeNet_without_linear(in_channels=1)\n",
    "\n",
    "    resnet_filename = f\"best_variant_lenet_model_1.pt\"\n",
    "    finetune_model = torch.load(resnet_filename)\n",
    "    model_path = f\"best_finetune_variant_lenet_model_{fold_index}.pt\"\n",
    "    solver = Solver(\n",
    "        dataloaders, model, model_path, \"cuda\", n_classes, finetune_model\n",
    "    )\n",
    "    \n",
    "    train_accuracy, val_accuracy, useful_stuff = solver.train(500)\n",
    "    C, y_true, y_pred, test_accuracy, y_pred_prob = solver.test()\n",
    "    train_avg_accuracy.append(train_accuracy)\n",
    "    val_avg_accuracy.append(val_accuracy)\n",
    "    avg_accuracy.append(np.round(test_accuracy,4))\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    for i in range(np.unique(y_true).shape[0]):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_test == i, y_pred_prob[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "    values = [\n",
    "        v\n",
    "        for v in roc_auc.values()\n",
    "        if isinstance(v, (int, float)) and not math.isnan(v)\n",
    "    ]\n",
    "    if values:\n",
    "        auc_score = sum(values) / len(values)\n",
    "    avg_roc.append(auc_score)\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=ORDER)\n",
    "    sns.set_context(\"talk\", rc={\"font\": \"Helvetica\", \"font.size\": 12})\n",
    "    label = [STRAINS[i] for i in ORDER]\n",
    "    cm = 100 * cm / cm.sum(axis=1)[:,np.newaxis]\n",
    "\n",
    "    avg_recall.append(recall_score(y_true, y_pred, average='macro', zero_division=0).round(4))\n",
    "    avg_specificity.append(specificity_score(y_true, y_pred, average='weighted').round(4))\n",
    "    avg_precision.append(precision_score(y_true, y_pred, average='weighted', zero_division=0).round(4))\n",
    "    avg_f1.append(f1_score(y_true, y_pred, average='weighted', zero_division=0).round(4))\n",
    "    df = pd.DataFrame(\n",
    "        {\n",
    "            \"Accuracy\": [np.round(accuracy_score(y_true, y_pred), 4)],\n",
    "            \"Recall\": [\n",
    "                recall_score(y_true, y_pred, average=None, zero_division=0).round(4)\n",
    "            ],\n",
    "            \"Specificity\": [specificity_score(y_true, y_pred, average=None).round(4)],\n",
    "            \"Precision\": [\n",
    "                precision_score(y_true, y_pred, average=None, zero_division=0).round(4)\n",
    "            ],\n",
    "            \"F1 Score\": [\n",
    "                f1_score(y_true, y_pred, average=None, zero_division=0).round(4)\n",
    "            ],\n",
    "        }\n",
    "    )\n",
    "    print(df.transpose())\n",
    "\n",
    "    plot_ROC_curve(\"variant_lenet\", y_true, y_test, y_pred_prob, fold_index=fold_index)\n",
    "    plot_heatmap(\"variant_lenet\", cm, fold_index=fold_index)\n",
    "    plot_metrics(training_results=useful_stuff, fold_index=fold_index, fold_name=\"variant_lenet\")\n",
    "    plot_loss_metrics(training_results=useful_stuff, fold_index=fold_index, fold_name=\"variant_lenet\")\n",
    "    acc = plot_antibiotic_groupings(\"variant_lenet\", y_true, y_pred, fold_index=fold_index)\n",
    "    antibiotic_accuracy.append(acc)\n",
    "    fold_index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8533, 0.8573, 0.8537, 0.8563, 0.861]\n",
      "Recall [0.8533, 0.8573, 0.8537, 0.8563, 0.861]\n",
      "Specificity [0.9949, 0.9951, 0.995, 0.995, 0.9952]\n",
      "Precision [0.862, 0.8643, 0.8607, 0.8636, 0.8685]\n",
      "F1 [0.8492, 0.8542, 0.8499, 0.8527, 0.8572]\n",
      "train mean: 1.0\n",
      "train std: 0.0\n",
      "val mean: 0.9413\n",
      "val std: 0.0016\n",
      "test mean: 0.8563\n",
      "test std: 0.0028\n",
      "recall mean: 0.8563\n",
      "recall std: 0.0028\n",
      "Specificity mean: 0.995\n",
      "Specificity std: 0.0001\n",
      "Precision mean: 0.8638\n",
      "Precision std: 0.0027\n",
      "F1 mean: 0.8526\n",
      "F1 std: 0.0029\n",
      "auc mean: 0.9937\n",
      "auc std: 0.0002\n",
      "antibiotic mean: 0.9730666666666666\n",
      "antibiotic std: 0.000489897948556624\n"
     ]
    }
   ],
   "source": [
    "print(avg_accuracy)\n",
    "print('Recall', avg_recall)\n",
    "print('Specificity', avg_specificity)\n",
    "print('Precision', avg_precision)\n",
    "print('F1', avg_f1)\n",
    "\n",
    "print(\"train mean:\", round(np.mean(train_avg_accuracy),4))\n",
    "print(\"train std:\", round(np.std(train_avg_accuracy),4))\n",
    "\n",
    "print(\"val mean:\", round(np.mean(val_avg_accuracy),4))\n",
    "print(\"val std:\", round(np.std(val_avg_accuracy),4))\n",
    "\n",
    "print(\"test mean:\", round(np.mean(avg_accuracy),4))\n",
    "print(\"test std:\", round(np.std(avg_accuracy),4))\n",
    "\n",
    "print(\"recall mean:\", round(np.mean(avg_recall),4))\n",
    "print(\"recall std:\", round(np.std(avg_recall),4))\n",
    "\n",
    "print(\"Specificity mean:\", round(np.mean(avg_specificity),4))\n",
    "print(\"Specificity std:\", round(np.std(avg_specificity),4))\n",
    "\n",
    "print(\"Precision mean:\", round(np.mean(avg_precision),4))\n",
    "print(\"Precision std:\", round(np.std(avg_precision),4))\n",
    "\n",
    "print(\"F1 mean:\", round(np.mean(avg_f1),4))\n",
    "print(\"F1 std:\", round(np.std(avg_f1),4))\n",
    "\n",
    "print(\"auc mean:\", round(np.mean(avg_roc),4))\n",
    "print(\"auc std:\", round(np.std(avg_roc),4))\n",
    "\n",
    "print(\"antibiotic mean:\", np.mean(antibiotic_accuracy))\n",
    "print(\"antibiotic std:\", np.std(antibiotic_accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
