{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-30 21:29:19.107809: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-30 21:29:19.107885: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-30 21:29:19.109054: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "from boxsers.preprocessing import savgol_smoothing, cosmic_filter\n",
    "\n",
    "def preprocessing_method(x):\n",
    "    # 1) Applies a median filter to remove cosmic rays from the spectrum(s).\n",
    "    x = cosmic_filter(x, ks=3)\n",
    "    # 2) Smoothes the spectra\n",
    "    x = savgol_smoothing(x, 7, p=3, degree=0)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from boxsers.misc_tools import data_split\n",
    "\n",
    "df = pd.read_hdf('./data/Bile_acids_27_07_2020.h5', key='df')\n",
    "spectrum = df.iloc[:, 1:].to_numpy()\n",
    "spectrum = preprocessing_method(spectrum)  \n",
    "spectrum_label = df.loc[:, 'Classes']\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "numeric_labels = label_encoder.fit_transform(spectrum_label)\n",
    "(sp_train, sp_test, lab_train, lab_test) = data_split(spectrum, numeric_labels, b_size=0.15, rdm_ste=1, print_report=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.Variant_LeNet_without_linear import Variant_LeNet_without_linear\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torch.autograd import Variable\n",
    "from functools import partial\n",
    "from deep_SLDA import slda_loss, SLDA\n",
    "from imblearn.metrics import specificity_score\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "import torch.optim as optim\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import math\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    auc,\n",
    "    roc_curve,\n",
    ")\n",
    "from plot import plot_bile_acids_ROC_curve, plot_bile_acids_heatmap, plot_loss_metrics, plot_metrics\n",
    "\n",
    "n_classes = 6\n",
    "batch_size = 4000\n",
    "\n",
    "train_avg_accuracy = []\n",
    "val_avg_accuracy = []\n",
    "avg_accuracy = []\n",
    "avg_roc = []\n",
    "C = np.zeros((6, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Solver:\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataloaders,\n",
    "        model,\n",
    "        model_path,\n",
    "        device,\n",
    "        n_classes,\n",
    "    ):\n",
    "        self.dataloaders = dataloaders\n",
    "        self.device = device\n",
    "        self.net = model\n",
    "        self.net = self.net.to(self.device)\n",
    "\n",
    "        self.criterion = partial(\n",
    "            slda_loss,\n",
    "            n_classes=n_classes,\n",
    "        )\n",
    "\n",
    "        self.optimizer = optim.Adam(self.net.parameters(), lr=1e-4, betas=(0.5, 0.999))\n",
    "        self.model_path = model_path\n",
    "        self.n_classes = n_classes\n",
    "        self.slda_layer = SLDA(self.n_classes)\n",
    "\n",
    "    def iterate(self, epoch, phase, scheduler=None):\n",
    "        if phase == \"train\":\n",
    "            self.net.train()\n",
    "        else:\n",
    "            self.net.eval()\n",
    "\n",
    "        dataloader = self.dataloaders[phase]\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        loss_total = 0\n",
    "\n",
    "        # if phase == \"train\":\n",
    "        #     self.optimizer.zero_grad()\n",
    "\n",
    "        for batch_idx, (inputs, targets) in enumerate(dataloader):\n",
    "            inputs = Variable(inputs).to(self.device)\n",
    "            targets= Variable(targets).to(self.device)\n",
    "\n",
    "            feas = self.net(inputs)\n",
    "\n",
    "            if phase == \"train\":\n",
    "                dirs, range_eigenvalue, null_eigenvalue = self.slda_layer.fit(feas, targets, phase)\n",
    "                Z = torch.matmul(feas, dirs.T)\n",
    "                self.clf = LinearDiscriminantAnalysis()\n",
    "                self.clf.fit(Z.detach().data.cpu().numpy(),targets.cpu().numpy())\n",
    "                outputs = self.clf.predict(Z.detach().data.cpu().numpy())\n",
    "                outputs = torch.from_numpy(outputs).to(self.device)\n",
    "                loss = self.criterion(range_eigenvalue, null_eigenvalue)\n",
    "                self.dirs = dirs\n",
    "            else:\n",
    "                range_eigenvalue, null_eigenvalue = self.slda_layer.fit(feas, targets, phase)\n",
    "                Z = torch.matmul(feas, self.dirs.T)\n",
    "                outputs = self.clf.predict(Z.detach().data.cpu().numpy())\n",
    "                outputs = torch.from_numpy(outputs).to(self.device)\n",
    "                loss = self.criterion(range_eigenvalue, null_eigenvalue)\n",
    "                \n",
    "            if phase == \"train\":\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total += targets.size(0)\n",
    "            loss_total += 1\n",
    "            correct += outputs.eq(targets).cpu().sum().item()    \n",
    "        \n",
    "        avg_loss = total_loss / loss_total\n",
    "        total_acc = correct / total\n",
    "\n",
    "        return avg_loss, total_acc\n",
    "\n",
    "    def train(self, epochs):\n",
    "\n",
    "        best_acc = 0\n",
    "\n",
    "        useful_stuff = {\n",
    "            \"training_loss\": [],\n",
    "            \"validation_loss\": [],\n",
    "            \"train_metrics\": [],\n",
    "            \"validation_metrics\": [],\n",
    "        }\n",
    "\n",
    "        # lambda1 = lambda epoch: 0.9 ** (epoch // 20) if epoch >= 20 else 1.0\n",
    "        # self.scheduler = torch.optim.lr_scheduler.LambdaLR(self.optimizer, lr_lambda=lambda1)\n",
    "\n",
    "        for epoch in tqdm(range(epochs)):\n",
    "            \n",
    "            train_loss, train_acc = self.iterate(epoch, \"train\")\n",
    "            useful_stuff[\"training_loss\"].append(train_loss)\n",
    "            useful_stuff[\"train_metrics\"].append(train_acc)\n",
    "\n",
    "            # self.optimizer.step()\n",
    "            # self.scheduler.step()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                val_loss, val_acc = self.iterate(epoch, \"val\")\n",
    "                useful_stuff[\"validation_loss\"].append(val_loss)\n",
    "                useful_stuff[\"validation_metrics\"].append(val_acc)\n",
    "\n",
    "            \n",
    "            if val_acc > best_acc or epoch == 0:\n",
    "                best_acc = val_acc\n",
    "                checkpoint = {\n",
    "                    \"epoch\": epoch,\n",
    "                    \"val_loss\": val_loss,\n",
    "                    \"dirs\": self.dirs,\n",
    "                    \"clf\": self.clf,\n",
    "                    \"state_dict\": self.net.state_dict(),\n",
    "                }\n",
    "                torch.save(checkpoint, self.model_path)\n",
    "            \n",
    "        return train_acc, best_acc, useful_stuff\n",
    "\n",
    "    def test_iterate(self, epoch, phase):\n",
    "        self.net.eval()\n",
    "        dataloader = self.dataloaders[phase]\n",
    "        y_pred = []\n",
    "        y_true = []\n",
    "        y_pred_prob = []\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (inputs, targets) in enumerate(dataloader):\n",
    "                inputs, targets = Variable(inputs.cuda()), Variable(\n",
    "                    targets.cuda()\n",
    "                )\n",
    "                feas = self.net(inputs)\n",
    "                Z = torch.matmul(feas, self.dirs.T)\n",
    "                outputs = self.clf.predict(Z.detach().data.cpu().numpy())\n",
    "                outputs = torch.from_numpy(outputs).to(self.device)\n",
    "                outputs_prob = self.clf.predict_proba(Z.detach().data.cpu().numpy())\n",
    "                outputs_prob = torch.from_numpy(outputs_prob).to(self.device)\n",
    "\n",
    "                y_pred.append(outputs.cpu().numpy().ravel())\n",
    "                y_true.append(targets.cpu().numpy())\n",
    "                y_pred_prob.append(outputs_prob.cpu().numpy())\n",
    "            pass\n",
    "\n",
    "        y_pred_prob = np.concatenate(y_pred_prob)\n",
    "        y_pred = np.concatenate(y_pred)\n",
    "        y_true = np.concatenate(y_true)\n",
    "\n",
    "        return (\n",
    "            np.array(y_pred).flatten(),\n",
    "            np.array(y_true).flatten(),\n",
    "            np.array(y_pred_prob).reshape(720, 6),\n",
    "        )\n",
    "\n",
    "    def test(self):\n",
    "        checkpoint = torch.load(self.model_path)\n",
    "        epoch = checkpoint[\"epoch\"]\n",
    "        val_loss = checkpoint[\"val_loss\"]\n",
    "        self.dirs = checkpoint[\"dirs\"]\n",
    "        self.clf = checkpoint[\"clf\"]\n",
    "\n",
    "        self.net.load_state_dict(checkpoint[\"state_dict\"])\n",
    "        print(\"load model at epoch {}, with val loss: {:.3f}\".format(epoch, val_loss))\n",
    "        y_pred, y_true, y_pred_prob = self.test_iterate(epoch, \"test\")\n",
    "        print(\"total\", accuracy_score(y_true, y_pred))\n",
    "        for i in range(self.n_classes):\n",
    "            idx = y_true == i\n",
    "            print(\"class\", i, accuracy_score(y_true[idx], y_pred[idx]))\n",
    "\n",
    "        return (\n",
    "            confusion_matrix(y_true, y_pred),\n",
    "            y_true,\n",
    "            y_pred,\n",
    "            accuracy_score(y_true, y_pred),\n",
    "            y_pred_prob,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold: 1\n",
      "train size:  3264\n",
      "validation size:  816\n",
      "test size:  720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/500 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/500 [00:03<25:38,  3.08s/it]/home/luhung/.venv/torch2/lib/python3.10/site-packages/scipy/sparse/linalg/_eigen/arpack/arpack.py:1600: RuntimeWarning: k >= N for N * N square matrix. Attempting to use scipy.linalg.eigh instead.\n",
      "  warnings.warn(\"k >= N for N * N square matrix. \"\n",
      "  0%|          | 2/500 [00:05<21:31,  2.59s/it]/home/luhung/.venv/torch2/lib/python3.10/site-packages/scipy/sparse/linalg/_eigen/arpack/arpack.py:1600: RuntimeWarning: k >= N for N * N square matrix. Attempting to use scipy.linalg.eigh instead.\n",
      "  warnings.warn(\"k >= N for N * N square matrix. \"\n",
      "  1%|          | 3/500 [00:07<20:08,  2.43s/it]/home/luhung/.venv/torch2/lib/python3.10/site-packages/scipy/sparse/linalg/_eigen/arpack/arpack.py:1600: RuntimeWarning: k >= N for N * N square matrix. Attempting to use scipy.linalg.eigh instead.\n",
      "  warnings.warn(\"k >= N for N * N square matrix. \"\n",
      "  1%|          | 4/500 [00:09<19:29,  2.36s/it]/home/luhung/.venv/torch2/lib/python3.10/site-packages/scipy/sparse/linalg/_eigen/arpack/arpack.py:1600: RuntimeWarning: k >= N for N * N square matrix. Attempting to use scipy.linalg.eigh instead.\n",
      "  warnings.warn(\"k >= N for N * N square matrix. \"\n",
      "  1%|          | 5/500 [00:12<19:13,  2.33s/it]/home/luhung/.venv/torch2/lib/python3.10/site-packages/scipy/sparse/linalg/_eigen/arpack/arpack.py:1600: RuntimeWarning: k >= N for N * N square matrix. Attempting to use scipy.linalg.eigh instead.\n",
      "  warnings.warn(\"k >= N for N * N square matrix. \"\n",
      "100%|██████████| 500/500 [19:15<00:00,  2.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load model at epoch 446, with val loss: 4.096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 0.975\n",
      "class 0 0.959349593495935\n",
      "class 1 0.9401709401709402\n",
      "class 2 0.9739130434782609\n",
      "class 3 0.9910714285714286\n",
      "class 4 0.9849624060150376\n",
      "class 5 1.0\n",
      "                                                         0\n",
      "Accuracy                                             0.975\n",
      "Recall        [0.9593, 0.9402, 0.9739, 0.9911, 0.985, 1.0]\n",
      "Specificity  [0.9983, 0.9934, 0.9934, 0.9868, 0.9983, 1.0]\n",
      "Precision    [0.9916, 0.9649, 0.9655, 0.9328, 0.9924, 1.0]\n",
      "F1 Score      [0.9752, 0.9524, 0.9697, 0.961, 0.9887, 1.0]\n",
      "fold: 2\n",
      "train size:  3264\n",
      "validation size:  816\n",
      "test size:  720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [19:07<00:00,  2.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load model at epoch 470, with val loss: 3.941\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 0.9805555555555555\n",
      "class 0 0.967479674796748\n",
      "class 1 0.9743589743589743\n",
      "class 2 0.991304347826087\n",
      "class 3 0.9732142857142857\n",
      "class 4 0.9774436090225563\n",
      "class 5 1.0\n",
      "                                                         0\n",
      "Accuracy                                            0.9806\n",
      "Recall       [0.9675, 0.9744, 0.9913, 0.9732, 0.9774, 1.0]\n",
      "Specificity    [0.9966, 0.995, 0.995, 0.9934, 0.9966, 1.0]\n",
      "Precision    [0.9835, 0.9744, 0.9744, 0.9646, 0.9848, 1.0]\n",
      "F1 Score     [0.9754, 0.9744, 0.9828, 0.9689, 0.9811, 1.0]\n",
      "fold: 3\n",
      "train size:  3264\n",
      "validation size:  816\n",
      "test size:  720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [19:15<00:00,  2.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load model at epoch 392, with val loss: 4.096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 0.975\n",
      "class 0 0.983739837398374\n",
      "class 1 0.9401709401709402\n",
      "class 2 0.9826086956521739\n",
      "class 3 0.9821428571428571\n",
      "class 4 0.9699248120300752\n",
      "class 5 0.9916666666666667\n",
      "                                                            0\n",
      "Accuracy                                                0.975\n",
      "Recall       [0.9837, 0.9402, 0.9826, 0.9821, 0.9699, 0.9917]\n",
      "Specificity      [0.9983, 0.995, 0.9934, 0.9852, 0.9983, 1.0]\n",
      "Precision       [0.9918, 0.9735, 0.9658, 0.9244, 0.9923, 1.0]\n",
      "F1 Score      [0.9878, 0.9565, 0.9741, 0.9524, 0.981, 0.9958]\n",
      "fold: 4\n",
      "train size:  3264\n",
      "validation size:  816\n",
      "test size:  720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/500 [00:02<20:37,  2.48s/it]/home/luhung/.venv/torch2/lib/python3.10/site-packages/scipy/sparse/linalg/_eigen/arpack/arpack.py:1600: RuntimeWarning: k >= N for N * N square matrix. Attempting to use scipy.linalg.eigh instead.\n",
      "  warnings.warn(\"k >= N for N * N square matrix. \"\n",
      "  0%|          | 2/500 [00:04<19:54,  2.40s/it]/home/luhung/.venv/torch2/lib/python3.10/site-packages/scipy/sparse/linalg/_eigen/arpack/arpack.py:1600: RuntimeWarning: k >= N for N * N square matrix. Attempting to use scipy.linalg.eigh instead.\n",
      "  warnings.warn(\"k >= N for N * N square matrix. \"\n",
      "  1%|          | 3/500 [00:07<19:34,  2.36s/it]/home/luhung/.venv/torch2/lib/python3.10/site-packages/scipy/sparse/linalg/_eigen/arpack/arpack.py:1600: RuntimeWarning: k >= N for N * N square matrix. Attempting to use scipy.linalg.eigh instead.\n",
      "  warnings.warn(\"k >= N for N * N square matrix. \"\n",
      "  1%|          | 4/500 [00:09<19:18,  2.33s/it]/home/luhung/.venv/torch2/lib/python3.10/site-packages/scipy/sparse/linalg/_eigen/arpack/arpack.py:1600: RuntimeWarning: k >= N for N * N square matrix. Attempting to use scipy.linalg.eigh instead.\n",
      "  warnings.warn(\"k >= N for N * N square matrix. \"\n",
      "  1%|          | 5/500 [00:11<19:11,  2.33s/it]/home/luhung/.venv/torch2/lib/python3.10/site-packages/scipy/sparse/linalg/_eigen/arpack/arpack.py:1600: RuntimeWarning: k >= N for N * N square matrix. Attempting to use scipy.linalg.eigh instead.\n",
      "  warnings.warn(\"k >= N for N * N square matrix. \"\n",
      "  1%|          | 6/500 [00:14<19:02,  2.31s/it]/home/luhung/.venv/torch2/lib/python3.10/site-packages/scipy/sparse/linalg/_eigen/arpack/arpack.py:1600: RuntimeWarning: k >= N for N * N square matrix. Attempting to use scipy.linalg.eigh instead.\n",
      "  warnings.warn(\"k >= N for N * N square matrix. \"\n",
      "100%|██████████| 500/500 [19:13<00:00,  2.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load model at epoch 227, with val loss: 4.063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 0.975\n",
      "class 0 0.975609756097561\n",
      "class 1 0.9743589743589743\n",
      "class 2 0.9826086956521739\n",
      "class 3 0.9553571428571429\n",
      "class 4 0.9624060150375939\n",
      "class 5 1.0\n",
      "                                                         0\n",
      "Accuracy                                             0.975\n",
      "Recall       [0.9756, 0.9744, 0.9826, 0.9554, 0.9624, 1.0]\n",
      "Specificity  [0.9966, 0.9934, 0.9884, 0.9934, 0.9983, 1.0]\n",
      "Precision     [0.9836, 0.9661, 0.9417, 0.964, 0.9922, 1.0]\n",
      "F1 Score     [0.9796, 0.9702, 0.9617, 0.9596, 0.9771, 1.0]\n",
      "fold: 5\n",
      "train size:  3264\n",
      "validation size:  816\n",
      "test size:  720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [19:14<00:00,  2.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load model at epoch 357, with val loss: 3.963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 0.9763888888888889\n",
      "class 0 0.975609756097561\n",
      "class 1 0.9572649572649573\n",
      "class 2 0.9652173913043478\n",
      "class 3 0.9910714285714286\n",
      "class 4 0.9699248120300752\n",
      "class 5 1.0\n",
      "                                                         0\n",
      "Accuracy                                            0.9764\n",
      "Recall       [0.9756, 0.9573, 0.9652, 0.9911, 0.9699, 1.0]\n",
      "Specificity   [0.9966, 0.995, 0.9934, 0.9901, 0.9966, 1.0]\n",
      "Precision    [0.9836, 0.9739, 0.9652, 0.9487, 0.9847, 1.0]\n",
      "F1 Score     [0.9796, 0.9655, 0.9652, 0.9694, 0.9773, 1.0]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from datasets_spectrum import spectral_dataloader\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "fold_index = 1\n",
    "\n",
    "for train_idx, valid_idx in kfold.split(sp_train, lab_train):\n",
    "\n",
    "    print(\"fold:\", fold_index)\n",
    "    x_train, y_train = sp_train[train_idx], lab_train[train_idx]\n",
    "    x_valid, y_valid = sp_train[valid_idx], lab_train[valid_idx]\n",
    "    \n",
    "    print(\"train size: \", len(x_train))\n",
    "    print(\"validation size: \", len(x_valid))\n",
    "    print(\"test size: \", len(sp_test))\n",
    "\n",
    "    dl_tr = spectral_dataloader(\n",
    "        x_train, y_train, idxs=None, batch_size=batch_size, shuffle=True\n",
    "    )\n",
    "    dl_val = spectral_dataloader(\n",
    "        x_valid, y_valid, idxs=None, batch_size=batch_size, shuffle=False\n",
    "    )\n",
    "    dl_test = spectral_dataloader(sp_test, lab_test, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    values, counts = np.unique(np.asarray(lab_test), return_counts=True)\n",
    "\n",
    "    dataloaders = {\"train\": dl_tr, \"val\": dl_val, \"test\": dl_test}\n",
    "    model = Variant_LeNet_without_linear(in_channels=1)\n",
    "\n",
    "    model_path = f\"best_bile_acids_variant_lenet_model_{fold_index}.pt\"\n",
    "    solver = Solver(\n",
    "        dataloaders, model, model_path, \"cuda\", n_classes\n",
    "    )\n",
    "    \n",
    "    train_accuracy, val_accuracy, useful_stuff = solver.train(500)\n",
    "    C, y_true, y_pred, test_accuracy, y_pred_prob = solver.test()\n",
    "    train_avg_accuracy.append(train_accuracy)\n",
    "    val_avg_accuracy.append(val_accuracy)\n",
    "    avg_accuracy.append(np.round(test_accuracy,4))\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    for i in range(np.unique(y_true).shape[0]):\n",
    "        fpr[i], tpr[i], _ = roc_curve(lab_test == i, y_pred_prob[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "    values = [\n",
    "        v\n",
    "        for v in roc_auc.values()\n",
    "        if isinstance(v, (int, float)) and not math.isnan(v)\n",
    "    ]\n",
    "    if values:\n",
    "        auc_score = sum(values) / len(values)\n",
    "    avg_roc.append(auc_score)\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0,1,2,3,4,5])\n",
    "    sns.set_context(\"talk\", rc={\"font\": \"Helvetica\", \"font.size\": 12})\n",
    "    label = [\"Blank\", \"CA\", \"DCA\", \"GCDCA\", \"LCA\", \"TCDCA\"]\n",
    "    cm = 100 * cm / cm.sum(axis=1)[:,np.newaxis]\n",
    "\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    sensitivity = recall_score(y_true, y_pred, average=\"micro\", zero_division=0)\n",
    "    specificity = cm[1, 1] / (cm[1, 0] + cm[1, 1])\n",
    "    f1 = f1_score(y_true, y_pred, average=\"micro\", zero_division=0)\n",
    "\n",
    "    df = pd.DataFrame(\n",
    "        {\n",
    "            \"Accuracy\": [np.round(accuracy_score(y_true, y_pred), 4)],\n",
    "            \"Recall\": [\n",
    "                recall_score(y_true, y_pred, average=None, zero_division=0).round(4)\n",
    "            ],\n",
    "            \"Specificity\": [specificity_score(y_true, y_pred, average=None).round(4)],\n",
    "            \"Precision\": [\n",
    "                precision_score(y_true, y_pred, average=None, zero_division=0).round(4)\n",
    "            ],\n",
    "            \"F1 Score\": [\n",
    "                f1_score(y_true, y_pred, average=None, zero_division=0).round(4)\n",
    "            ],\n",
    "        }\n",
    "    )\n",
    "    print(df.transpose())\n",
    "\n",
    "    plot_bile_acids_ROC_curve(\"variant_lenet\", y_true, lab_test, y_pred_prob, fold_index=fold_index)\n",
    "    plot_bile_acids_heatmap(\"variant_lenet\", cm, fold_index=fold_index)\n",
    "    plot_metrics(training_results=useful_stuff, fold_index=fold_index, fold_name=\"variant_lenet\")\n",
    "    plot_loss_metrics(training_results=useful_stuff, fold_index=fold_index, fold_name=\"variant_lenet\")\n",
    "\n",
    "    fold_index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.975, 0.9806, 0.975, 0.975, 0.9764]\n",
      "train mean: 0.9971\n",
      "train std: 0.0014\n",
      "val mean: 0.973\n",
      "val std: 0.0046\n",
      "test mean: 0.9764\n",
      "test std: 0.0022\n",
      "auc mean: 0.9991\n",
      "auc std: 0.0003\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "print(avg_accuracy)\n",
    "print(\"train mean:\", round(np.mean(train_avg_accuracy),4))\n",
    "print(\"train std:\", round(np.std(train_avg_accuracy),4))\n",
    "\n",
    "print(\"val mean:\", round(np.mean(val_avg_accuracy),4))\n",
    "print(\"val std:\", round(np.std(val_avg_accuracy),4))\n",
    "\n",
    "print(\"test mean:\", round(np.mean(avg_accuracy),4))\n",
    "print(\"test std:\", round(np.std(avg_accuracy),4))\n",
    "\n",
    "print(\"auc mean:\", round(np.mean(avg_roc),4))\n",
    "print(\"auc std:\", round(np.std(avg_roc),4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
